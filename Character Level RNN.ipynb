{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "average-worse",
   "metadata": {},
   "source": [
    "## Character-level RNN \n",
    "<b>The text file supplements/anna.txt contains Leo Tolstoy's novel <i>Anna Karenina</i>.\n",
    "We train a character-level language model using this dataset.</b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49b698d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 83 distinct characters in the dataset.\n"
     ]
    }
   ],
   "source": [
    "# Warning: Rerunning this code block will create\n",
    "# a list of 'chars' w/ a different order/sequencing every time (I only realised\n",
    "# this after training my model, so I can't change this. If this is changed, forward passing\n",
    "# on the trained model will produce garbage output)\n",
    "\n",
    "# This means that you will get a unique idx_to_char mapping\n",
    "# every time you run this block. This will render forward passes through a previously trained\n",
    "# LSTM useless if you lose the original idx_to_char mapping that the model\n",
    "# was trained on. E.g. A model trained w/ idx_to_char = {1: 'x', 2: 'y', 3:'k'}\n",
    "# can only perform forward passes w/ the same idx_to_char dictionary. \n",
    "# A forward pass w/ a trained model on a new dictionary {1: 'y', 2: 'k', 3:'x'}\n",
    "# will produce garbage output!\n",
    "\n",
    "# A more robust solution would be to loop over all chars in anna.txt sequentially,\n",
    "# adding unique chars to a list to ensure that chars has the same order of characters\n",
    "# every time you run this code cell (implement this in future scenarios).\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# data I/0 \n",
    "data = open('supplements/anna.txt', 'r').read()\n",
    "\n",
    "# Data is a string. Converted into a set removes duplicates. \n",
    "# Then we store the characters in a list.\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print(f\"There are {vocab_size} distinct characters in the dataset.\")\n",
    "\n",
    "# Function used to map characters w/ a unique index\n",
    "# to a onehot vector (we map chars to idxs below in a dictionary)\n",
    "def idx2onehot(idx):\n",
    "    \"\"\"\n",
    "    Maps an idx 'int' to a one-hot-vector of length vocab size\n",
    "    \"\"\"\n",
    "    # Create a one-hot vector to represent the input.\n",
    "    x = torch.zeros((vocab_size, 1), dtype = torch.double)\n",
    "    x[idx] = 1\n",
    "    return x.double()\n",
    "\n",
    "# Mapping each unique character to a number (char_to_idx)\n",
    "# and a number to an idx (idx_to_char).\n",
    "# The numbers start from 0 to len(unique_chars) - 1 (i.e. 0 to 82 here for anna.txt)\n",
    "char_to_idx = {char:idx for idx, char in enumerate(chars)}\n",
    "idx_to_char = {idx:char for idx, char in enumerate(chars)}\n",
    "\n",
    "# Dictionary mapping an idx to a onehot vector of shape (vocab_size,1)\n",
    "idx_to_onehot = {idx:idx2onehot(idx) for idx in idx_to_char}\n",
    "\n",
    "# Just getting indexes of characters\n",
    "data_indxs = [char_to_idx[char] for char in data]\n",
    "del data # delete data from memory once finished with it\n",
    "\n",
    "# Length of our sequences used to train our LSTM\n",
    "seq_length = 20\n",
    "\n",
    "def create_dataset(data_indxs, seq_length):\n",
    "    '''\n",
    "    Transforms a list of indxs (which map to letters sequentially in anna.txt)\n",
    "    to torch.tensors X and y which form our training examples and ground truth labels.\n",
    "    \n",
    "    Args:\n",
    "        data_indxs: a list of indxs (which map to letters sequentially in anna.txt)\n",
    "        seq_length: length of the sequences used to train our LSTM\n",
    "        \n",
    "    Returns: \n",
    "        X is an n_sequences x seq_length training matrix.\n",
    "        Y has the same dimensions. \n",
    "    '''\n",
    "    \n",
    "    X, y = [], []\n",
    "    \n",
    "    i = 0\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        # Stop adding sequences to our training sets\n",
    "        # when we've reached the end of anna.txt\n",
    "        if i+seq_length+1 >= len(data_indxs):\n",
    "            break\n",
    "\n",
    "        feature = data_indxs[i:i+seq_length]\n",
    "        target = data_indxs[i+1:i+seq_length+1]\n",
    "        \n",
    "        # X is a list of lists of torch.doubles\n",
    "        # which are our training sequences\n",
    "        X.append(feature)\n",
    "        \n",
    "        # Y is a list of lists of torch.doubles\n",
    "        # which are our training target sequences \n",
    "        # These are the sequences in X shifted by 1\n",
    "        y.append(target)\n",
    "        \n",
    "        # Reduce i to i += 1 if\n",
    "        # you want to slide sequences along 1 character\n",
    "        # and produce more training data.\n",
    "        i+=25\n",
    "        \n",
    "    return torch.tensor(X, dtype = torch.double), torch.tensor(y, dtype = torch.double)\n",
    "\n",
    "# x_train_RNN, y_train_RNN \n",
    "x_train_RNN, y_train_RNN = create_dataset(data_indxs, seq_length)\n",
    "del data_indxs # delete data_indxs once we have our dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chubby-terminal",
   "metadata": {},
   "source": [
    "<b>\n",
    "Below I describe the LSTM RNN architecture that I used for this language model and explain the hyperparameters chosen (e.g. how many layers are there, what are the activation functions used).</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "federal-vinyl",
   "metadata": {},
   "source": [
    "I used an LSTM architecture for this task. The RNN has an 83-dimensional input layer, $x_{t}$, and an 83-dimensional output layer $y_{t}$ at each timestep, as well as a hidden layer $h_t$ of 50 neurons. Note that $h_t$ is a function of $o_{t} = \\sigma(W_{o} \\dot [h_{t-1},x_{t}] +b_o)$ and $c_{t}(c_{t-1}, f_{t}, i_{t}, \\tilde c_t)$, where $o_{t}, f_{t}, i_{t}$ and $\\tilde c_t$ are all single layered MLPs  that perform a linear transformation on $x_t$ and $h_{t-1}$. The equations below summarise the LSTM architecture in its entirety:\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "f_{t} &= \\sigma (W_{f}[h_{t-1},x_{t}] +b_f)  \\\\\n",
    "i_{t} &= \\sigma (W_{i}[h_{t-1},x_{t}] +b_i)   \\\\  \n",
    "o_{t} &= \\sigma (W_{o}[h_{t-1},x_{t}] +b_o) \\\\     \n",
    "\\tilde c_{t} &= tanh (W_{h}[h_{t-1},x_{t}] +b_h) \\\\\n",
    "c_{t} &= f_t \\odot c_{t-1} + i_{t} \\odot \\tilde c_{t}\\\\\n",
    "h_{t} &= o_{t} \\odot tanh(c_t) \\\\\n",
    "y_{t} &= W_{y}h_{t} + b_{y}\\\\\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "Note that $y_{t}$ is an 83-dimensional output vector of 'logits' which, during training, is passed into the cross-entropy loss function. However, during inference or sequence forecasting, if we want to compute say the next $n$ characters that follow a string with $m$ characters, we update $h_t$ using the first $m$ characters in the string (which correspond to $m$ inputs $x_t$), without computing $y_t$. There is no harm computing $y_t$ here, but it is not needed. Thereafter, we take the value of $y_t$ outputted by the $m^{th}$ character; compute $p_t = softmax(y_t)$ and feed in the 83-dimensional one-hot encoded $argmax(p_t)$ as the input into $h_{t+1}$ in the following timestep. We repeat this process for as long as desired to generate a forecasted sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a78ceed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Converts a (n_batches, 1) tensor --> (n_batches, 83) \n",
    "# tensor of one_hot encodings\n",
    "def col2onehot(col, idx_to_onehot):\n",
    "\n",
    "    # Convert onehot to a torch tensor of dtype = int64/long\n",
    "    return torch.tensor([list(idx_to_onehot[idx.item()]) for idx in col])    \n",
    "\n",
    "class AnnaLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size = 83, hidden_size = 50):\n",
    "        super(AnnaLSTM, self).__init__()\n",
    "        \n",
    "        self.LSTM_input_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.Linear_output_size = vocab_size\n",
    "\n",
    "        self.lstm = nn.LSTMCell(self.LSTM_input_size, self.hidden_size, dtype = torch.double)\n",
    "        self.linear = nn.Linear(self.hidden_size, self.Linear_output_size, dtype = torch.double) \n",
    "        # We compute softmax along the columns of our (n_batches, seq_length) LSTM output\n",
    "        # vector i.e. dim = 1\n",
    "        self.softmax = nn.Softmax(dim = 1)\n",
    "\n",
    "    def forward(self, x, y): \n",
    "        '''\n",
    "        Forward pass of LSTM\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x: a tensor of shape (n_samples, seq_length) containing indexes (that map to letters via idx_to_char) \n",
    "        of examples. MUST convert numbers to one-hot encodings within forward pass loop.\n",
    "        y: a tensor of shape (n_samples, seq_length) containing indexes (which map to letters via idx_to_char)\n",
    "        of targets. MUST convert numbers to one-hot encodings within forward pass loop.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        outputs: a tensor of shape (n_samples*seq_length, vocab_size)\n",
    "           - These are all of the logits stacked in rows, W/ the output\n",
    "           of the 1st character of the first example on the first row, and the output\n",
    "           of the last character of the last example on the last row\n",
    "           \n",
    "        groundtruths: a tensor of shape (n_samples*seq_length, vocab_size)\n",
    "           - These are all of the one-hot-encoded targets stacked in rows, W/ the target\n",
    "           of the 1st character of the first example on the first row, and the target\n",
    "           of the last character of the last example on the last row\n",
    "        '''\n",
    "\n",
    "        outputs, groundtruths = [], []\n",
    "        \n",
    "        # Getting batch_size and sequence length\n",
    "        batch_size, seq_length = x.shape\n",
    "\n",
    "        # c_t and h_t are initialized w/ zeros and \n",
    "        # shape = (batch_size, hidden_size)\n",
    "        h_t = torch.zeros(batch_size, self.hidden_size, dtype=torch.double).to(x.device)\n",
    "        c_t = torch.zeros(batch_size, self.hidden_size, dtype=torch.double).to(x.device)\n",
    "\n",
    "        # Here, x.chunk(x.size(1)), dim = 1) splits out (n_batches, seq_length)\n",
    "        # training block up into seq_length columns - so we can effectively\n",
    "        # forward pass n_batches sequences at once.\n",
    "        for i, input_t in enumerate(x.chunk(x.size(1), dim=1)):\n",
    "            \n",
    "            # input_t is a (n_batches, 1) column vector of idxs that needs\n",
    "            # to be converted to onehot vectors i.e. need (n_batches, 1) --> (n_batches, 83)\n",
    "            input_t = col2onehot(input_t, idx_to_onehot)\n",
    "            \n",
    "            # y.chunk(y.shape[1],dim = 1) returns a tuple of columns of y \n",
    "            # since y is a (n_batches, 1) column vector of idxs that need\n",
    "            # to be converted to onehot vectors, we need (n_batches, 1) --> (n_batches, 83)\n",
    "            groundtruth_t = col2onehot(y.chunk(y.shape[1], dim = 1)[i], idx_to_onehot)\n",
    "            \n",
    "            h_t, c_t = self.lstm(input_t, (h_t, c_t))\n",
    "            logits = self.linear(h_t)\n",
    "            \n",
    "            outputs += [logits]\n",
    "            groundtruths += [groundtruth_t]\n",
    "            \n",
    "        # Outputs are of shape (n_batches*seq_length, 83)\n",
    "        outputs = torch.stack(outputs, dim = 1).reshape(batch_size*seq_length,-1)\n",
    "        # Associated groundtruths are of shape (n_batches*seq_length, 83)\n",
    "        groundtruths = torch.stack(groundtruths, dim = 1).reshape(batch_size*seq_length,-1)\n",
    "\n",
    "        return outputs, groundtruths\n",
    "    \n",
    "    def sample(self, x, future=50):\n",
    "        '''\n",
    "        Forward pass of LSTM\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x: a tensor of shape (n_samples, seq_length) containing indexes (letters) of to-be-forecasted sequences\n",
    "           MUST convert numbers to one-hot encodings within forward pass loop\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        outputs: a tensor of shape (n_samples, future) containing indexes which map to characters\n",
    "                 via char_to_idx/idx_to_char defined above.\n",
    "        '''\n",
    "        best_idxs_output = []\n",
    "        \n",
    "        batch_size, seq_length = x.shape\n",
    "        \n",
    "        # c_t and h_t are initialized w/ zeros and \n",
    "        # shape = (batch_size, hidden_size)\n",
    "        h_t = torch.zeros(batch_size, self.hidden_size, dtype=torch.double).to(x.device)\n",
    "        c_t = torch.zeros(batch_size, self.hidden_size, dtype=torch.double).to(x.device)\n",
    "\n",
    "        # Here, x.chunk(x.size(1)), dim = 1) splits out (n_batches, seq_length)\n",
    "        # training block up into seq_length columns - so we can effectively\n",
    "        # forward pass n_batches sequences at once\n",
    "        \n",
    "        for i, input_t in enumerate(x.chunk(x.size(1), dim=1)):\n",
    "            \n",
    "            # input_t is a (n_batches, 1) column vector of idxs that needs\n",
    "            # to be converted to onehot vectors i.e. need (n_batches, 1) --> (n_batches, 83)\n",
    "            input_t = col2onehot(input_t, idx_to_onehot)\n",
    "            h_t, c_t = self.lstm(input_t, (h_t, c_t))\n",
    "            \n",
    "            if i == (x.size(1) - 1):\n",
    "                logits = self.linear(h_t)\n",
    "                p_t = self.softmax(logits)\n",
    "\n",
    "                # This produces a column vector of shape (n_batches, 1)\n",
    "                # Need to transform to  ---> (n_batches, 83)\n",
    "                best_idxs = torch.argmax(p_t, dim = 1)\n",
    "                best_idxs_output += [best_idxs]\n",
    "                output_t = col2onehot(best_idxs, idx_to_onehot)\n",
    "                \n",
    "        # Keep feeding in outputs into the lstm\n",
    "        for i in range(future-1):\n",
    "\n",
    "            # STARTING FROM THE LAST OUTPUT - KEEP UPDATING OUTPUT TO FORECAST INTO FUTURE\n",
    "            h_t, c_t = self.lstm(output_t, (h_t, c_t))\n",
    "            logits = self.linear(h_t)\n",
    "            p_t = self.softmax(logits)\n",
    "\n",
    "            # This produces a column vector of shape (n_batches, 1)\n",
    "            # Need to transform to  ---> (n_batches, 83)\n",
    "            best_idxs = torch.argmax(p_t, dim = 1)\n",
    "            best_idxs_output += [best_idxs]\n",
    "            output_t = col2onehot(best_idxs, idx_to_onehot)\n",
    "        \n",
    "        # Here we concatenate the tensors along the column dimension\n",
    "        outputs = torch.stack(best_idxs_output, 1).squeeze()\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appointed-exclusive",
   "metadata": {},
   "source": [
    "<b>Below I train my language model on the dataset.\n",
    "I also describe details in the training procedure. </b>\n",
    "\n",
    "<b>Furthermore, I show the training progress by reporting how the result of using my RNN to\n",
    "complete the sentence ``The meaning of life is'' changes as more training is\n",
    "done.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "416e2438",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "def trainRNN(net, x, y, lr=0.5, momentum=0.9, batch_size=64, nepochs=20):\n",
    "    \n",
    "    # Define the device the net parameters are on\n",
    "    device = next(net.parameters()).device \n",
    "    optimizer = optim.SGD(net.parameters(), lr=lr, momentum=momentum)\n",
    "    \n",
    "    # Cross Entropy Loss - loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    dataloader = DataLoader(DatasetWrapper(x, y), batch_size=batch_size, shuffle=True)\n",
    "    loop = tqdm(range(nepochs))\n",
    "    \n",
    "    # training loop\n",
    "    for i in loop: # for each epoch\n",
    "        t0 = time()\n",
    "        \n",
    "        epoch_loss = 0\n",
    "        n_batches = 0\n",
    "        for (x_batch, y_batch) in dataloader: # for each mini-batch\n",
    "            \n",
    "            # Move mini batches to device (GPU if one exists)\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            \n",
    "            # Zero gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Get outputs and groundtruths in desired shape/form\n",
    "            # through net forward pass\n",
    "            outputs, groundtruths = net(x_batch, y_batch)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, groundtruths)\n",
    "            \n",
    "            # Get gradients\n",
    "            loss.backward()\n",
    "            \n",
    "            # Update parameters\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Add loss to epoch loss to eventually compute\n",
    "            # an average epoch loss\n",
    "            # Detach from the computational graph to avoid RAM increasing spuriously\n",
    "            epoch_loss += loss.detach()\n",
    "            n_batches += 1\n",
    "      \n",
    "        # Average epoch loss\n",
    "        epoch_loss = epoch_loss/n_batches\n",
    "\n",
    "        # show training progress\n",
    "        loop.set_postfix(Epoch_Loss ='|%7.5f|' % epoch_loss.item(), time='|%7.2f|' % (time()-t0))\n",
    "        \n",
    "        # Show training progress after each epoch\n",
    "        with torch.no_grad(): \n",
    "            test_string = \"The meaning of life is\"\n",
    "            input_sample = torch.tensor([char_to_idx[char] for char in test_string]).unsqueeze(0)\n",
    "            sample_idxs = net.sample(input_sample, future = 50)\n",
    "            txt = ''.join(idx_to_char[idx.item()] for idx in sample_idxs)\n",
    "            output = test_string + txt\n",
    "            print(f\"---- Progress after Epoch {i+1}\\n{output} \\n----\")\n",
    "            \n",
    "        # Save model\n",
    "        torch.save(net.state_dict(), f\"supplements/RNNstate_dict_epoch{i+1}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8a983060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Progress after Epoch 2\n",
      "The meaning of life is the said the said the said the said the said the  \n",
      "----\n",
      "---- Progress after Epoch 3\n",
      "The meaning of life is the said, and the said, and the said, and the sai \n",
      "----\n",
      "---- Progress after Epoch 6\n",
      "The meaning of life is to the stalked the stalked the stalked the stalke \n",
      "----\n",
      "---- Progress after Epoch 14\n",
      "The meaning of life is the controady of the same the same the same the s \n",
      "----\n",
      "---- Progress after Epoch 18\n",
      "The meaning of life is the countess the same to the same to the same to  \n",
      "----\n",
      "---- Progress after Epoch 19\n",
      "The meaning of life is the strain of the strain of the strain of the str \n",
      "----\n",
      "---- Progress after Epoch 20\n",
      "The meaning of life is the continual to her for the same to the same to  \n",
      "----\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0) # for reproducibility\n",
    "from util import *\n",
    "\n",
    "# Boolean flag which decides if we want\n",
    "# to retrain our model, or just load saved model\n",
    "# weights and produce necessary outputs\n",
    "RETRAIN = False\n",
    "\n",
    "if RETRAIN:\n",
    "    device = (\n",
    "        \"cuda\"\n",
    "        if torch.cuda.is_available()\n",
    "        else \"mps\"\n",
    "        if torch.backends.mps.is_available()\n",
    "        else \"cpu\"\n",
    "    )\n",
    "\n",
    "    net = AnnaLSTM().to(device)\n",
    "    trainRNN(net, x_train_RNN, y_train_RNN, lr=0.5, momentum=0.9, batch_size=64, nepochs=20)\n",
    "    \n",
    "else:\n",
    "    \n",
    "    epochs = [2,3,6,14,18,19,20] #include more but these were the only interesting ones\n",
    "    \n",
    "    for epoch in epochs:\n",
    "        \n",
    "        loaded_RNN = AnnaLSTM()\n",
    "        loaded_RNN.load_state_dict(torch.load(f\"supplements/RNNstate_dict_epoch{epoch}.pth\"))\n",
    "        \n",
    "        # Show training progress after each selected epoch\n",
    "        with torch.no_grad(): \n",
    "            test_string = \"The meaning of life is\"\n",
    "            input_sample = torch.tensor([char_to_idx[char] for char in test_string]).unsqueeze(0)\n",
    "            sample_idxs = loaded_RNN.sample(input_sample, future = 50)\n",
    "            txt = ''.join(idx_to_char[idx.item()] for idx in sample_idxs)\n",
    "            output = test_string + txt\n",
    "            print(f\"---- Progress after Epoch {epoch}\\n{output} \\n----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8dfb37c",
   "metadata": {},
   "source": [
    "I trained the LSTM architecture using an SGD optimizer with momentum = 0.9, learning rate = 0.5 and batch size = 64. The model was trained for 20 epochs (as I ran out of time). The initial hidden state $h_{t_0}$ and cell state $c_{t_0}$ were simply zero vectors. Furthermore, each training sequence had a length of 20. It should also be noted that the training set was generated by grabbing the first 20 characters in the anna.txt file, and using it as our first training example. The corresponding target or ground truth label was a string of 20 characters 'shifted' along by 1 step. Subsequent training examples and training targets were generated by sliding along 25 characters and then again storing training examples and targets of length 20. This procedure gave us a training set with 79,409 training examples. In an ideal scenario, however, we would have generated training examples by sliding along 1 character at a time rather than 25. This would have resulted in a training set with 1,985,201 example sequences of length 20.\n",
    "\n",
    "Above we can see how the model completed the sentence \"The meaning of life is\" after selected epochs. Note that the model predicted the next 50 characters following that initial partial sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grave-perfume",
   "metadata": {},
   "source": [
    "<b>Below I comment on the text generated by my character-level RNN. I also discuss what my model has learned to generate, and its limitations.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55add25f",
   "metadata": {},
   "source": [
    "Referring to the model's output above after selected epochs, the model appears to improve its ability to generate realistic text as training continues. At early epochs, the model correctly includes spaces between its words; however, it repeats one or two words constantly (which is obviously not good). As training continues, the model starts to include a greater variety of 'words' (note that some words aren't actually words in the English language). Since I was unable to train the model for longer than 20 epochs, the final model does not generate realistic text. However, I am sure that 100+ epochs of training would have produced a better model.\n",
    "\n",
    "Some possible limitations of the model include the fact that it was trained on a reduced training set as mentioned above. Furthermore, I didn't have time to trial different combinations of learning rates, batch sizes, epochs, momentum values as well as various combinations of sequence lengths to train the model on. In addition to this, there was no rigorous or precise way to evaluate the quality of the model's output at test time (e.g. we could't evaluate the quality of its output using some kind of test loss metric). The only metric we had was our sense of the English language in general, or by monitoring the training set loss each epoch (which in fact decreased monotonically for the most part).\n",
    "\n",
    "Another limitation of using an LSTM for a character-level language model is that such models have a limited context window, meaning that they can only consider a fixed number of characters before and after the current character. This makes it challenging for the model to capture long-range dependencies and understand the larger context of the text. Our character-level LSTM model also does not capture the semantic meaning of the words and sentences, which can limit its ability to generate coherent and meaningful text. This can be especially problematic when generating longer sequences of text."
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
